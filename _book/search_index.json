[["comparing-two-means.html", "PHCM9795 Foundations of Biostatistics Chapter 1 Comparing two means 1.1 Checking data for the independent samples t-test 1.2 Independent samples t-test 1.3 Checking the assumptions for a Paired t-test 1.4 Paired t-Test", " PHCM9795 Foundations of Biostatistics Notes for R Term 2, 2022 Chapter 1 Comparing two means 1.1 Checking data for the independent samples t-test 1.1.1 Producing histograms and boxplots by a second variable We can create histograms and boxplots separated by a second variable in (at least) two ways: using Base R or ggplot2 graphics. We will demonstrate using the birthweight data in Example_5.1.dta. library(haven) library(labelled) library(skimr) library(ggplot2) library(jmv) bwt &lt;- read_dta(&quot;data/examples/Example_5.1.dta&quot;) %&gt;% unlabelled() skim(bwt) Table 1.1: Data summary Name bwt Number of rows 100 Number of columns 2 _______________________ Column type frequency: factor 1 numeric 1 ________________________ Group variables None Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts gender 0 1 FALSE 2 Fem: 56, Mal: 44 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist birthweight 0 1 3.51 0.37 2.75 3.26 3.45 3.77 4.25 ▂▅▇▅▃ summary(bwt$gender) #&gt; Female Male #&gt; 56 44 To use Base R graphics, we can create subsets of the birthweight data, subsetted for males and females separately. Note here that gender is a factor, so we need to select based on the factor labels, not the underlying numeric code. bwt_m &lt;- subset(bwt, bwt$gender==&quot;Male&quot;) bwt_f &lt;- subset(bwt, bwt$gender==&quot;Female&quot;) We can now create hisotgrams and boxplots for males and females separately, in the usual way. To place the graphs next to each other in a single figure, we can use the par function. The par function sets the graphics parameters. Essentially, we want to tell R to split a plot window into a matrix with nr rows and nc columns, and we can decide to fill the cells by rows (mfrow) or columns (mfcols). For example, to plot four figures in a single plot, filled by rows, we use par(mfrow=c(2,2)). When we are done plotting multiple graphs, we can reset the plot window by submitting par(mfrow=c(1,1)). par(mfrow=c(2,2)) hist(bwt_m$birthweight, xlim=c(2.5, 4.5), xlab=&quot;Birthweight (kg)&quot;, main=&quot;Males&quot;) hist(bwt_f$birthweight, xlim=c(2.5, 4.5), xlab=&quot;Birthweight (kg)&quot;, main=&quot;Females&quot;) boxplot(bwt_m$birthweight, ylim=c(2.5, 4.5), ylab=&quot;Birthweight (kg)&quot;, main=&quot;Males&quot;) boxplot(bwt_f$birthweight, ylim=c(2.5, 4.5), ylab=&quot;Birthweight (kg)&quot;, main=&quot;Females&quot;) par(mfrow=c(1,1)) To produce separate histograms in ggplot2, we use the facet_wrap function to create a grid of plots. We can define the variable(s) to be plotted by in the vars(), and optionally, the number of rows (nrow=) and number of columns (ncol=). # Overall histogram of birthweight ggplot(bwt, aes(x=birthweight)) + geom_histogram(breaks=seq(2.5, 4.5, 0.25), colour=&quot;black&quot;, fill=&quot;grey&quot;) + labs(x=&quot;Birthweight (kg)&quot;, y=&quot;Frequency&quot;) + theme_classic() # Histogram by gender ggplot(bwt, aes(x=birthweight)) + geom_histogram(breaks=seq(2.5, 4.5, 0.25), colour=&quot;black&quot;, fill=&quot;grey&quot;) + facet_wrap(vars(gender), nrow=1, ncol=2) + labs(x=&quot;Birthweight (kg)&quot;, y=&quot;Frequency&quot;) + theme_classic() While it is possible to use facet_wrap to produce separate boxplots, we can use the fact that the boxplot allows an x variable to be assigned to the ggplot aesthetic. By defining birthweight as the y variable and gender as the x variable, we can produce two boxplots in the same figure: ggplot(bwt, aes(x=gender, y=birthweight)) + geom_boxplot() + labs(y=&quot;Birthweight (kg)&quot;, x=&quot;Gender&quot;) + theme_classic() 1.1.2 Producing split summary statistics The descriptives function within the jmv function allows summary statistics to be calculated within subgroups using the splitBy argument: descriptives(data=bwt, vars=birthweight, splitBy=gender) #&gt; #&gt; DESCRIPTIVES #&gt; #&gt; Descriptives #&gt; ─────────────────────────────────────────────── #&gt; gender birthweight #&gt; ─────────────────────────────────────────────── #&gt; N Female 56 #&gt; Male 44 #&gt; Missing Female 0 #&gt; Male 0 #&gt; Mean Female 3.587411 #&gt; Male 3.421364 #&gt; Median Female 3.530000 #&gt; Male 3.430000 #&gt; Standard deviation Female 0.3629788 #&gt; Male 0.3536165 #&gt; Minimum Female 2.950000 #&gt; Male 2.750000 #&gt; Maximum Female 4.250000 #&gt; Male 4.100000 #&gt; ─────────────────────────────────────────────── 1.2 Independent samples t-test ttestIS(data=bwt, vars=birthweight, group=gender) #&gt; #&gt; INDEPENDENT SAMPLES T-TEST #&gt; #&gt; Independent Samples T-Test #&gt; ──────────────────────────────────────────────────────────────────── #&gt; Statistic df p #&gt; ──────────────────────────────────────────────────────────────────── #&gt; birthweight Student&#39;s t 2.296556 98.00000 0.0237731 #&gt; ──────────────────────────────────────────────────────────────────── ttestIS(data=bwt, vars=birthweight, group=gender, meanDiff=TRUE, ci=TRUE) #&gt; #&gt; INDEPENDENT SAMPLES T-TEST #&gt; #&gt; Independent Samples T-Test #&gt; ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── #&gt; Statistic df p Mean difference SE difference Lower Upper #&gt; ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── #&gt; birthweight Student&#39;s t 2.296556 98.00000 0.0237731 0.1660471 0.07230265 0.02256481 0.3095293 #&gt; ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── ttestIS(data=bwt, vars=birthweight, group=gender, meanDiff=TRUE, ci=TRUE, welchs=TRUE) #&gt; #&gt; INDEPENDENT SAMPLES T-TEST #&gt; #&gt; Independent Samples T-Test #&gt; ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── #&gt; Statistic df p Mean difference SE difference Lower Upper #&gt; ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── #&gt; birthweight Student&#39;s t 2.296556 98.00000 0.0237731 0.1660471 0.07230265 0.02256481 0.3095293 #&gt; Welch&#39;s t 2.303840 93.54377 0.0234458 0.1660471 0.07207403 0.02293328 0.3091609 #&gt; ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── 1.3 Checking the assumptions for a Paired t-test Before performing a paired t-test, you must check that the assumptions for the test have been met. Using the dataset Example_5.2.dta to show that the difference between the pair of measurements between the sites is normally distributed, we first need to compute a new variable of the differences and examine its histogram. sbp &lt;- read_dta(&quot;/Users/td/Documents/GithubRepos/phcm9795/data/examples/Example_5.2.dta&quot;) sbp$diff = sbp$sbp_dp - sbp$sbp_tp hist(sbp$diff, xlab=&quot;Blood pressure (mmHg)&quot;, main=&quot;Difference in systolic blood pressure&quot;) 1.4 Paired t-Test To perform a paired t-test we will use the dataset Example_5.2.dta. ttestPS(data=sbp, pairs=list(list(i1 = &#39;sbp_dp&#39;, i2 = &#39;sbp_tp&#39;)), meanDiff=TRUE, ci=TRUE, norm=TRUE) #&gt; #&gt; PAIRED SAMPLES T-TEST #&gt; #&gt; Paired Samples T-Test #&gt; ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── #&gt; statistic df p Mean difference SE difference Lower Upper #&gt; ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── #&gt; sbp_dp sbp_tp Student&#39;s t -0.9621117 106.0000 0.3381832 -1.261682 1.311368 -3.861596 1.338232 #&gt; ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────── #&gt; #&gt; #&gt; Normality Test (Shapiro-Wilk) #&gt; ─────────────────────────────────────────────────── #&gt; W p #&gt; ─────────────────────────────────────────────────── #&gt; sbp_dp - sbp_tp 0.8984529 0.0000006 #&gt; ─────────────────────────────────────────────────── #&gt; Note. A low p-value suggests a violation of #&gt; the assumption of normality "],["proportions.html", "Chapter 2 Proportions 2.1 95% confidence intervals for proportions 2.2 Significance test for single proportion 2.3 Computing a relative risk and its 95% confidence interval 2.4 Computing an odds ratio and its 95%CI", " Chapter 2 Proportions 2.1 95% confidence intervals for proportions We can use the BinomCI(x=, n=, method=) function within the DescTools package to compute 95% confidence intervals for proportions. Here we specify x: the number of successes, n: the sample size, and optionally, the method (which defaults to Wilson’s method). library(DescTools) BinomCI(x=47, n=215, method=&#39;wald&#39;) #&gt; est lwr.ci upr.ci #&gt; [1,] 0.2186047 0.1633595 0.2738498 BinomCI(x=47, n=215, method=&#39;wilson&#39;) #&gt; est lwr.ci upr.ci #&gt; [1,] 0.2186047 0.1685637 0.2785246 2.2 Significance test for single proportion We can use the binom.test function to perform a significance test for a single proportion: binom.test(x=, n=, p=). Here we specify x: the number of successes, n: the sample size, and p: the hypothesised proportion (which defaults to 0.5 if nothing is entered). binom.test(x=54, n=300, p=0.2) #&gt; #&gt; Exact binomial test #&gt; #&gt; data: 54 and 300 #&gt; number of successes = 54, number of trials = 300, #&gt; p-value = 0.4273 #&gt; alternative hypothesis: true probability of success is not equal to 0.2 #&gt; 95 percent confidence interval: #&gt; 0.1382104 0.2282394 #&gt; sample estimates: #&gt; probability of success #&gt; 0.18 2.3 Computing a relative risk and its 95% confidence interval library(haven) library(labelled) library(jmv) drug &lt;- read_dta(&quot;/Users/td/Documents/GithubRepos/phcm9795/data/examples/Example_6.4.dta&quot;) %&gt;% unlabelled() head(drug) #&gt; # A tibble: 6 × 2 #&gt; group side_effect #&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 Placebo Nausea #&gt; 2 Placebo Nausea #&gt; 3 Placebo Nausea #&gt; 4 Placebo Nausea #&gt; 5 Placebo No nausea #&gt; 6 Placebo No nausea table(drug$group) #&gt; #&gt; Placebo Active #&gt; 50 50 table(drug$side_effect) #&gt; #&gt; No nausea Nausea #&gt; 81 19 table(drug$group, drug$side_effect) #&gt; #&gt; No nausea Nausea #&gt; Placebo 46 4 #&gt; Active 35 15 drug$group &lt;- relevel(drug$group, ref=&quot;Active&quot;) drug$side_effect &lt;- relevel(drug$side_effect, ref=&quot;Nausea&quot;) table(drug$group) #&gt; #&gt; Active Placebo #&gt; 50 50 table(drug$side_effect) #&gt; #&gt; Nausea No nausea #&gt; 19 81 table(drug$group, drug$side_effect) #&gt; #&gt; Nausea No nausea #&gt; Active 15 35 #&gt; Placebo 4 46 contTables(data=drug, rows=group, cols=side_effect, pcRow=TRUE, relRisk = TRUE, diffProp = TRUE) #&gt; #&gt; CONTINGENCY TABLES #&gt; #&gt; Contingency Tables #&gt; ────────────────────────────────────────────────────────────────── #&gt; group Nausea No nausea Total #&gt; ────────────────────────────────────────────────────────────────── #&gt; Active Observed 15 35 50 #&gt; % within row 30.00000 70.00000 100.00000 #&gt; #&gt; Placebo Observed 4 46 50 #&gt; % within row 8.00000 92.00000 100.00000 #&gt; #&gt; Total Observed 19 81 100 #&gt; % within row 19.00000 81.00000 100.00000 #&gt; ────────────────────────────────────────────────────────────────── #&gt; #&gt; #&gt; χ² Tests #&gt; ───────────────────────────────────── #&gt; Value df p #&gt; ───────────────────────────────────── #&gt; χ² 7.862248 1 0.0050478 #&gt; N 100 #&gt; ───────────────────────────────────── #&gt; #&gt; #&gt; Comparative Measures #&gt; ─────────────────────────────────────────────────────────────────────── #&gt; Value Lower Upper #&gt; ─────────────────────────────────────────────────────────────────────── #&gt; Difference in 2 proportions 0.2200000 0.07238986 0.3676101 #&gt; Relative risk 3.750000 1.337540 10.51370 #&gt; ─────────────────────────────────────────────────────────────────────── If you only have the cross-tabulated data (i.e. aggregated), you will need to enter your data into a new data frame. drug_aggregated &lt;- data.frame( group = c(1, 1, 0, 0), side_effect = c(1, 0, 1, 0), n = c(15, 35, 4, 46) ) drug_aggregated$group &lt;- factor(drug_aggregated$group, levels=c(1,0), labels=c(&quot;Active&quot;, &quot;Placebo&quot;)) drug_aggregated$side_effect &lt;- factor(drug_aggregated$side_effect, levels=c(1,0), labels=c(&quot;Nausea&quot;, &quot;No nausea&quot;)) drug_aggregated #&gt; group side_effect n #&gt; 1 Active Nausea 15 #&gt; 2 Active No nausea 35 #&gt; 3 Placebo Nausea 4 #&gt; 4 Placebo No nausea 46 contTables(data=drug_aggregated, rows=group, cols=side_effect, count=n, pcRow=TRUE, relRisk = TRUE, diffProp = TRUE) #&gt; #&gt; CONTINGENCY TABLES #&gt; #&gt; Contingency Tables #&gt; ────────────────────────────────────────────────────────────────── #&gt; group Nausea No nausea Total #&gt; ────────────────────────────────────────────────────────────────── #&gt; Active Observed 15 35 50 #&gt; % within row 30.00000 70.00000 100.00000 #&gt; #&gt; Placebo Observed 4 46 50 #&gt; % within row 8.00000 92.00000 100.00000 #&gt; #&gt; Total Observed 19 81 100 #&gt; % within row 19.00000 81.00000 100.00000 #&gt; ────────────────────────────────────────────────────────────────── #&gt; #&gt; #&gt; χ² Tests #&gt; ───────────────────────────────────── #&gt; Value df p #&gt; ───────────────────────────────────── #&gt; χ² 7.862248 1 0.0050478 #&gt; N 100 #&gt; ───────────────────────────────────── #&gt; #&gt; #&gt; Comparative Measures #&gt; ─────────────────────────────────────────────────────────────────────── #&gt; Value Lower Upper #&gt; ─────────────────────────────────────────────────────────────────────── #&gt; Difference in 2 proportions 0.2200000 0.07238986 0.3676101 #&gt; Relative risk 3.750000 1.337540 10.51370 #&gt; ─────────────────────────────────────────────────────────────────────── 2.4 Computing an odds ratio and its 95%CI We can use the contTables function To obtain an odds ratio and its 95% CI, by specifying odds=TRUE: hpv &lt;- data.frame( cancer = c(1, 1, 0, 0), hpv = c(1, 0, 1, 0), n = c(57, 14, 43, 186) ) hpv$cancer &lt;- factor(hpv$cancer, levels=c(1,0), labels=c(&quot;Case&quot;, &quot;Control&quot;)) hpv$hpv &lt;- factor(hpv$hpv, levels=c(1,0), labels=c(&quot;HPV +&quot;, &quot;HPV -&quot;)) hpv #&gt; cancer hpv n #&gt; 1 Case HPV + 57 #&gt; 2 Case HPV - 14 #&gt; 3 Control HPV + 43 #&gt; 4 Control HPV - 186 contTables(data=hpv, rows=hpv, cols=cancer, count=n, odds = TRUE) #&gt; #&gt; CONTINGENCY TABLES #&gt; #&gt; Contingency Tables #&gt; ───────────────────────────────────── #&gt; hpv Case Control Total #&gt; ───────────────────────────────────── #&gt; HPV + 57 43 100 #&gt; HPV - 14 186 200 #&gt; Total 71 229 300 #&gt; ───────────────────────────────────── #&gt; #&gt; #&gt; χ² Tests #&gt; ────────────────────────────────────── #&gt; Value df p #&gt; ────────────────────────────────────── #&gt; χ² 92.25660 1 &lt; .0000001 #&gt; N 300 #&gt; ────────────────────────────────────── #&gt; #&gt; #&gt; Comparative Measures #&gt; ────────────────────────────────────────────────── #&gt; Value Lower Upper #&gt; ────────────────────────────────────────────────── #&gt; Odds ratio 17.61130 8.992580 34.49041 #&gt; ────────────────────────────────────────────────── "],["testing-proportions.html", "Chapter 3 Testing proportions 3.1 Pearson’s chi-squared test 3.2 Chi-squared test for tables larger than 2-by-2 3.3 McNemar’s test for paired proportions", " Chapter 3 Testing proportions 3.1 Pearson’s chi-squared test 3.1.1 Individual data We will demonstrate how to use R to conduct a Pearson chi-squared test using Worked Example 7.1. library(haven) library(labelled) nausea &lt;- read_dta(&quot;/Users/td/Documents/GithubRepos/phcm9795/data/examples/Example_7.1.dta&quot;) %&gt;% unlabelled() head(nausea) #&gt; # A tibble: 6 × 2 #&gt; group side_effect #&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 Placebo Nausea #&gt; 2 Placebo Nausea #&gt; 3 Placebo Nausea #&gt; 4 Placebo Nausea #&gt; 5 Placebo No nausea #&gt; 6 Placebo No nausea These data have been labelled in Stata, and we use the unlabelled function to convert the labelled data into factors. We can confirm that the variables are stored as factors using the str function to examine the structure of the variables, and the table function to produce a frequency table. str(nausea$group) #&gt; Factor w/ 2 levels &quot;Placebo&quot;,&quot;Active&quot;: 1 1 1 1 1 1 1 1 1 1 ... #&gt; - attr(*, &quot;label&quot;)= chr &quot;Group&quot; table(nausea$group) #&gt; #&gt; Placebo Active #&gt; 50 50 str(nausea$side_effect) #&gt; Factor w/ 2 levels &quot;No nausea&quot;,&quot;Nausea&quot;: 2 2 2 2 1 1 1 1 1 1 ... #&gt; - attr(*, &quot;label&quot;)= chr &quot;Side effect&quot; table(nausea$side_effect) #&gt; #&gt; No nausea Nausea #&gt; 81 19 To conduct a chi-square test on these data, we first construct a table and view the expected frequencies. tab &lt;- table(nausea$group, nausea$side_effect) tab #&gt; #&gt; No nausea Nausea #&gt; Placebo 46 4 #&gt; Active 35 15 chisq.test(tab)$expected #&gt; #&gt; No nausea Nausea #&gt; Placebo 40.5 9.5 #&gt; Active 40.5 9.5 After confirming that there are no cells with small expected frequencies, we can conduct the chi-square test. By default, R conducts a chi-square test with a continuity correction. To obtain an identical result to that produced by Stata, we use the correct=FALSE statement. chisq.test(tab) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity #&gt; correction #&gt; #&gt; data: tab #&gt; X-squared = 6.4977, df = 1, p-value = 0.0108 chisq.test(tab, correct=FALSE) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: tab #&gt; X-squared = 7.8622, df = 1, p-value = 0.005048 The last line labelled Pearson chi2(1) reports the appropriate Chi-squared test statistic which has a value of 7.862 with 1 degree of freedom and a P value of 0.005. fisher.test(tab) #&gt; #&gt; Fisher&#39;s Exact Test for Count Data #&gt; #&gt; data: tab #&gt; p-value = 0.009489 #&gt; alternative hypothesis: true odds ratio is not equal to 1 #&gt; 95 percent confidence interval: #&gt; 1.384999 21.862717 #&gt; sample estimates: #&gt; odds ratio #&gt; 4.852862 3.1.2 Summarised data When you only have the cross-tabulated data, you can enter the summarised data manually. The TextToTable function in the DescTools library is useful here: library(DescTools) text &lt;- &quot; NoNausea, Nausea Placebo, 46, 4 ActiveDrug, 35, 15&quot; table &lt;- TextToTable(text, header=TRUE, sep=&quot;,&quot;, dimnames=c(&quot;Group&quot;, &quot;SideEffect&quot;)) chisq.test(table)$expected #&gt; SideEffect #&gt; Group NoNausea Nausea #&gt; Placebo 40.5 9.5 #&gt; ActiveDrug 40.5 9.5 chisq.test(table) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity #&gt; correction #&gt; #&gt; data: table #&gt; X-squared = 6.4977, df = 1, p-value = 0.0108 chisq.test(table, correct=FALSE) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: table #&gt; X-squared = 7.8622, df = 1, p-value = 0.005048 3.2 Chi-squared test for tables larger than 2-by-2 Use the data in Example_7.2.dta. We use similar steps as described above for a 2-by-2 table. allergy &lt;- read_dta(&quot;/Users/td/Documents/GithubRepos/phcm9795/data/examples/Example_7.2.dta&quot;) %&gt;% unlabelled() head(allergy) #&gt; # A tibble: 6 × 8 #&gt; id asthma hdmallergy catallergy infection sex #&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 1 No Yes No Yes Female #&gt; 2 2 Yes No No No Female #&gt; 3 3 Yes No No No Female #&gt; 4 4 No No No No Male #&gt; 5 4 Yes Yes Yes No Female #&gt; 6 5 Yes Yes Yes No Female #&gt; # … with 2 more variables: maternalasthma &lt;fct&gt;, #&gt; # allergy_severity &lt;fct&gt; tab_allergy &lt;- table(allergy$allergy_severity, allergy$sex) tab_allergy #&gt; #&gt; Female Male #&gt; Non-allergic 150 137 #&gt; Slight allergy 50 70 #&gt; Moderate allergy 27 32 #&gt; Severe allergy 15 19 chisq.test(tab_allergy)$expected #&gt; #&gt; Female Male #&gt; Non-allergic 138.908 148.092 #&gt; Slight allergy 58.080 61.920 #&gt; Moderate allergy 28.556 30.444 #&gt; Severe allergy 16.456 17.544 chisq.test(tab_allergy) #&gt; #&gt; Pearson&#39;s Chi-squared test #&gt; #&gt; data: tab_allergy #&gt; X-squared = 4.3089, df = 3, p-value = 0.23 jmv::contTables(allergy, allergy_severity, sex, pcCol=TRUE) #&gt; #&gt; CONTINGENCY TABLES #&gt; #&gt; Contingency Tables #&gt; ────────────────────────────────────────────────────────────────────────────── #&gt; allergy_severity Female Male Total #&gt; ────────────────────────────────────────────────────────────────────────────── #&gt; Non-allergic Observed 150 137 287 #&gt; % within column 61.98347 53.10078 57.40000 #&gt; #&gt; Slight allergy Observed 50 70 120 #&gt; % within column 20.66116 27.13178 24.00000 #&gt; #&gt; Moderate allergy Observed 27 32 59 #&gt; % within column 11.15702 12.40310 11.80000 #&gt; #&gt; Severe allergy Observed 15 19 34 #&gt; % within column 6.19835 7.36434 6.80000 #&gt; #&gt; Total Observed 242 258 500 #&gt; % within column 100.00000 100.00000 100.00000 #&gt; ────────────────────────────────────────────────────────────────────────────── #&gt; #&gt; #&gt; χ² Tests #&gt; ───────────────────────────────────── #&gt; Value df p #&gt; ───────────────────────────────────── #&gt; χ² 4.308913 3 0.2299813 #&gt; N 500 #&gt; ───────────────────────────────────── 3.3 McNemar’s test for paired proportions To perform this test in R, we will use the dataset Example_7.3.dta. drug &lt;- read_dta(&quot;/Users/td/Documents/GithubRepos/phcm9795/data/examples/Example_7.3.dta&quot;) %&gt;% unlabelled() head(drug) #&gt; # A tibble: 6 × 2 #&gt; druga drugb #&gt; &lt;fct&gt; &lt;fct&gt; #&gt; 1 Yes Yes #&gt; 2 Yes Yes #&gt; 3 Yes Yes #&gt; 4 Yes Yes #&gt; 5 Yes Yes #&gt; 6 Yes Yes Responses to each drug should be in separate variables in the dataset as shown in Table 7.2 using the tabulate2 command (Statistics &gt; Summaries, tables, and tests &gt; Frequency tables &gt; Two-way table with measures of association). In the tabulate2 dialog box, tick Relative frequencies under Cell contents as shown below. tab_drug &lt;- table(drug$druga, drug$drugb) tab_drug #&gt; #&gt; No Yes #&gt; No 5 14 #&gt; Yes 20 21 prop.table(tab_drug) #&gt; #&gt; No Yes #&gt; No 0.08333333 0.23333333 #&gt; Yes 0.33333333 0.35000000 To perform the McNemar’s test, go to Statistics &gt; Epidemiology and related &gt; Tables for epidemiologists &gt; Matched case-control studies. In the mcc dialog box, select the variable drugb as the Exposed case variable and druga as the Exposed control variable as shown below. mcnemar.test(tab_drug) #&gt; #&gt; McNemar&#39;s Chi-squared test with continuity #&gt; correction #&gt; #&gt; data: tab_drug #&gt; McNemar&#39;s chi-squared = 0.73529, df = 1, p-value = #&gt; 0.3912 epibasix::mcNemar(tab_drug)$rd #&gt; [1] -0.1 epibasix::mcNemar(tab_drug)$rd.CIL #&gt; [1] -0.3054528 epibasix::mcNemar(tab_drug)$rd.CIU #&gt; [1] 0.1054528 "],["correlation-and-simple-linear-regression.html", "Chapter 4 Correlation and simple linear regression 4.1 Creating a scatter plot 4.2 Calculating a correlation coefficient 4.3 Fitting a simple linear regression model 4.4 Plotting residuals from a simple linear regression", " Chapter 4 Correlation and simple linear regression We will demonstrate using Stata for correlation and simple linear regression using the dataset Example_8.1.dta. library(ggplot2) # Optional, for nicer looking scatterplots library(haven) # For importing data lung &lt;- read_dta(&quot;/Users/td/Documents/GithubRepos/phcm9795/data/examples/Example_8.1.dta&quot;) 4.1 Creating a scatter plot We can use the plot function to create a scatter plot to explore the association between height and FVC, assigning meaningful labels with the xlab and ylab commands: plot(x=lung$Height, y=lung$FVC, xlab=&quot;Height (cm)&quot;, ylab=&quot;Forced vital capacity (L)&quot;) To add a fitted line, we can use the abline() function which adds a straight line to the plot. The equation of this straight line will be determined from the estimated regression line, which we specify with lm(y ~ x). Putting this all together: plot(x=lung$Height, y=lung$FVC, xlab=&quot;Height (cm)&quot;, ylab=&quot;Forced vital capacity (L)&quot;) abline(lm(lung$FVC ~ lung$Height)) To create a scatter plot using ggplot2, we define the x and y aesthetics as the Height and FVC. We then specify that we want to plot points, by specifying the point geometry using geom_point. We can add labels in the usual way. Putting it all together: ggplot(data=lung, aes(x=Height, y=FVC)) + geom_point() + labs(x=&quot;Height (cm)&quot;, y=&quot;Forced vital capacity (L)&quot;) We can add an estimated regression line by adding a geom_smooth, specifying that the line should be based on a linear model (lm), and no error shading should be included (se=FALSE): ggplot(data=lung, aes(x=Height, y=FVC)) + geom_point() + geom_smooth(method=lm, se=FALSE) + labs(x=&quot;Height (cm)&quot;, y=&quot;Forced vital capacity (L)&quot;) #&gt; `geom_smooth()` using formula &#39;y ~ x&#39; 4.2 Calculating a correlation coefficient We can use the cor.test function to calculate a Pearson’s correlation coefficient: cor.test(lung$Height, lung$FVC) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: lung$Height and lung$FVC #&gt; t = 10.577, df = 118, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.5924715 0.7794090 #&gt; sample estimates: #&gt; cor #&gt; 0.697628 4.3 Fitting a simple linear regression model We can use the lm function to fit a simple linear regression model, specifying the model as y ~ x. Using Example_8.1.dta, we can quantify the relationship between FVC and height. lm(FVC ~ Height, data=lung) #&gt; #&gt; Call: #&gt; lm(formula = FVC ~ Height, data = lung) #&gt; #&gt; Coefficients: #&gt; (Intercept) Height #&gt; -18.8735 0.1408 The default output from the lm function is rather sparse. We can obtain much more useful information by defining the model as an object, then using the summary() function: model1 &lt;- lm(FVC ~ Height, data=lung) summary(model1) #&gt; #&gt; Call: #&gt; lm(formula = FVC ~ Height, data = lung) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -1.01139 -0.23643 -0.02082 0.24918 1.31786 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -18.87347 2.19365 -8.604 3.89e-14 *** #&gt; Height 0.14076 0.01331 10.577 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.3965 on 118 degrees of freedom #&gt; Multiple R-squared: 0.4867, Adjusted R-squared: 0.4823 #&gt; F-statistic: 111.9 on 1 and 118 DF, p-value: &lt; 2.2e-16 Finally, we can obtain 95% confidence intervals for the regression coefficients using the confint function: confint(model1) #&gt; 2.5 % 97.5 % #&gt; (Intercept) -23.2174967 -14.5294444 #&gt; Height 0.1144042 0.1671092 4.4 Plotting residuals from a simple linear regression We can use the resid function to obtain the residuals from a saved model. These residuals can then be plotted using a histogram in the usual way: residuals &lt;- resid(model1) hist(residuals) A Normal curve can be overlaid if we plot the residuals using a probability scale. hist(residuals, probability = TRUE, ylim=c(0,1)) curve(dnorm(x, mean=mean(residuals), sd=sd(residuals)), col=&quot;darkblue&quot;, lwd=2, add=TRUE) Alternatively, a ggplot2 approach can be used, after converting the single vector of residuals into a dataframe: resid &lt;- as.data.frame(residuals) ggplot(resid, aes(x=residuals)) + geom_histogram(binwidth = 0.5, boundary=-1.5, colour=&quot;black&quot;, fill=&quot;white&quot;) ggplot(resid, aes(x=residuals)) + geom_histogram(aes(y = ..density..), binwidth = 0.5, boundary=-1.5, colour=&quot;black&quot;, fill=&quot;white&quot;) + stat_function(fun = dnorm, args = list(mean = mean(resid$residuals), sd = sd(resid$residuals))) "],["analysing-non-normal-data.html", "Chapter 5 Analysing non-normal data 5.1 Transforming non-normally distributed variables 5.2 Wilcoxon ranked-sum test 5.3 Wilcoxon matched-pairs signed-rank test 5.4 Estimating rank correlation coefficients", " Chapter 5 Analysing non-normal data 5.1 Transforming non-normally distributed variables One option for dealing with a non-normally distributed varaible is to transform it into its square, square root or logarithmic value. The new transformed variable may be normally distributed and therefore a parametric test can be used. First we check the distribution of the variable for normality, e.g. by plotting a histogram. You can calculate a new, transformed, variable using variable transformation commands. For example, to create a new column of data based on the log of length of stay using Base R: library(haven) # For importing data library(skimr) library(labelled) hospital &lt;- read_dta(&quot;/Users/td/Documents/GithubRepos/phcm9795/data/examples/Example_9.1.dta&quot;) %&gt;% unlabelled() hospital$ln_los &lt;- log(hospital$los+1) skim(hospital) Table 1.1: Data summary Name hospital Number of rows 132 Number of columns 6 _______________________ Column type frequency: character 1 factor 2 numeric 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace gender 0 1 1 1 0 2 0 Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts infect 0 1 FALSE 2 No: 106, Yes: 26 surgery 0 1 FALSE 3 Car: 53, Abd: 48, Oth: 31 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist id 0 1 75.50 38.25 10 42.75 75.50 108.25 141.0 ▇▇▇▇▇ los 0 1 38.05 35.78 0 20.75 27.00 42.00 244.0 ▇▁▁▁▁ ln_los 0 1 3.41 0.71 0 3.08 3.33 3.76 5.5 ▁▁▇▇▂ A tidyverse version uses the mutate command to create a new variable: library(tidyverse) #&gt; ── Attaching packages ─────────────────── tidyverse 1.3.1 ── #&gt; ✓ ggplot2 3.3.5 ✓ purrr 0.3.4 #&gt; ✓ tibble 3.1.6 ✓ dplyr 1.0.8 #&gt; ✓ tidyr 1.2.0 ✓ stringr 1.4.0 #&gt; ✓ readr 2.1.2 ✓ forcats 0.5.1 #&gt; ── Conflicts ────────────────────── tidyverse_conflicts() ── #&gt; x dplyr::filter() masks stats::filter() #&gt; x dplyr::lag() masks stats::lag() hospital &lt;- read_dta(&quot;/Users/td/Documents/GithubRepos/phcm9795/data/examples/Example_9.1.dta&quot;) %&gt;% unlabelled() hospital &lt;- hospital %&gt;% mutate(ln_los = log(los+1)) skim(hospital) Table 5.1: Data summary Name hospital Number of rows 132 Number of columns 6 _______________________ Column type frequency: character 1 factor 2 numeric 3 ________________________ Group variables None Variable type: character skim_variable n_missing complete_rate min max empty n_unique whitespace gender 0 1 1 1 0 2 0 Variable type: factor skim_variable n_missing complete_rate ordered n_unique top_counts infect 0 1 FALSE 2 No: 106, Yes: 26 surgery 0 1 FALSE 3 Car: 53, Abd: 48, Oth: 31 Variable type: numeric skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist id 0 1 75.50 38.25 10 42.75 75.50 108.25 141.0 ▇▇▇▇▇ los 0 1 38.05 35.78 0 20.75 27.00 42.00 244.0 ▇▁▁▁▁ ln_los 0 1 3.41 0.71 0 3.08 3.33 3.76 5.5 ▁▁▇▇▂ You can now check whether this logged variable is normally distributed as described in Module 2, for example by plotting a histogram as shown in Figure 9.2. To obtain the back-transformed mean shown in Output 9.1, we can use the exp command: exp(3.407232) #&gt; [1] 30.18159 If your transformed variable is approximately normally distributed, you can apply parametric tests such as the t-test. In the Worked Example 9.1 dataset, the variable infect (presence of nosocomial infection) is a binary categorical variable. To test the hypothesis that patients with nosocomial infection have a different length of stay to patients without infection, you can conduct a t-test on the ln_los variable. You will need to back transform your mean values, as shown in Worked Example 9.1 in the course notes when reporting your results. 5.2 Wilcoxon ranked-sum test We use the wilcox.test function to perform the Wilcoxon ranked-sum test: wilcox.test(continuous_variable ~ group_variable, data=df, correct=FALSE) The Wilcoxon ranked-sum test will be demonstrated using the length of stay data in Example_9.1.dta. Here, out continuous variable is los and the grouping variable is infect. wilcox.test(los ~ infect, data=hospital, correct=FALSE) #&gt; #&gt; Wilcoxon rank sum test #&gt; #&gt; data: los by infect #&gt; W = 949, p-value = 0.01402 #&gt; alternative hypothesis: true location shift is not equal to 0 5.3 Wilcoxon matched-pairs signed-rank test The wilcox.test function can also be used to conduct the Wilcoxon matched-pairs signed-rank test. The specification of the variables is a little different, in that each variable is specified as dataframe$variable: wilcox.test(df$continuous_variable_1, df$continuous_variable_1, paired=TRUE) We will demonstrate using the dataset on the arthritis drug cross-over trial (Example_9.2.dta). Like the paired t-test the paired data need to be in separate columns. arthritis &lt;- read_dta(&quot;/Users/td/Documents/GithubRepos/phcm9795/data/examples/Example_9.2.dta&quot;) %&gt;% unlabelled() arthritis$difference = arthritis$drug_1 - arthritis$drug_2 hist(arthritis$difference, xlab=&quot;Difference&quot;, main=&quot;Histogram of differences in pain scores&quot;) wilcox.test(arthritis$drug_1, arthritis$drug_2, paired=TRUE) #&gt; Warning in wilcox.test.default(arthritis$drug_1, #&gt; arthritis$drug_2, paired = TRUE): cannot compute exact p- #&gt; value with ties #&gt; #&gt; Wilcoxon signed rank test with continuity correction #&gt; #&gt; data: arthritis$drug_1 and arthritis$drug_2 #&gt; V = 10.5, p-value = 0.04898 #&gt; alternative hypothesis: true location shift is not equal to 0 5.4 Estimating rank correlation coefficients The analyses for Spearman’s and Kendall’s rank correlation are conducted in similar ways: lung &lt;- read_dta(&quot;/Users/td/Documents/GithubRepos/phcm9795/data/examples/Example_8.1.dta&quot;) cor.test(lung$Height, lung$FVC, method=&quot;spearman&quot;) #&gt; Warning in cor.test.default(lung$Height, lung$FVC, method = #&gt; &quot;spearman&quot;): Cannot compute exact p-value with ties #&gt; #&gt; Spearman&#39;s rank correlation rho #&gt; #&gt; data: lung$Height and lung$FVC #&gt; S = 72699, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true rho is not equal to 0 #&gt; sample estimates: #&gt; rho #&gt; 0.7475566 cor.test(lung$Height, lung$FVC, method=&quot;kendall&quot;) #&gt; #&gt; Kendall&#39;s rank correlation tau #&gt; #&gt; data: lung$Height and lung$FVC #&gt; z = 8.8244, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true tau is not equal to 0 #&gt; sample estimates: #&gt; tau #&gt; 0.5609431 "],["sample-size.html", "Chapter 6 Sample size 6.1 Sample size calculation for two independent samples t-test 6.2 Sample size calculation for difference between two independent proportions 6.3 Sample size calculation with a relative risk 6.4 Sample size calculation with an odds ratio", " Chapter 6 Sample size Many power and sample size procedures are available in the epiR package. # If not yet installed, submit the following: # install.packages(&quot;epiR&quot;) library(epiR) 6.1 Sample size calculation for two independent samples t-test To do the problem discussed in Worked Example 10.2, we use the epi.sscompc function: Sample size, power and minimum detectable difference when comparing continuous outcomes. epi.sscompc(treat, control, n, sigma, power, r = 1, design = 1, sided.test = 2, nfractional = FALSE, conf.level = 0.95) The first line contains parameters that we can specify, with one parameter that is to be calculated. That is, we define values for four of the five parameters, and the remaining parameter is calculated. For example, we can define the mean in the treated group, the mean in the control group, the assumed standard deviation and the desired power, and the function will calculate the required sample size. We specify the unknown value as being equal to R’s missing value, NA. For example, to calculate the required sample size in Worked Example 10.2, we specify: the assumed mean in the experimental, or treatment, group: 90mmHg the assumed mean in the control group: 95mmHg the standard deviation of blood pressure: 5mmHg the required power, 0.9 (representing 90%) The values on the second line of the function are defined by default, and we can leave these as default. Putting this all together, and specifying the sample size as unknown: epi.sscompc(treat=90, control=95, n=NA, sigma=5, power=0.9) #&gt; $n.total #&gt; [1] 44 #&gt; #&gt; $n.treat #&gt; [1] 22 #&gt; #&gt; $n.control #&gt; [1] 22 #&gt; #&gt; $power #&gt; [1] 0.9 #&gt; #&gt; $delta #&gt; [1] 5 The results indicate that we need 22 participants in each group, or 44 in total. We can define whether we want unequal numbers in each group by specifying r: the number in the treatment group divided by the number in the control group. 6.2 Sample size calculation for difference between two independent proportions To do the problem discussed in Worked Example 10.3, we use the epi.sscohortc function: Sample size, power or minimum detectable incidence risk ratio for a cohort study using individual count data. epi.sscohortc(irexp1, irexp0, pexp = NA, n = NA, power = 0.80, r = 1, N, design = 1, sided.test = 2, finite.correction = FALSE, nfractional = FALSE, conf.level = 0.95) We can enter: irexp1: the assumed risk of the outcome in the exposed group: here 0.35 irexp0: the assumed risk of the outcome in the unexposed group: here 0.2 n: the total sample size, to be determined power: the required power: here 0.8 (representing 80%) epi.sscohortc(irexp1=0.35, irexp0=0.2, n=NA, power=0.8) #&gt; $n.total #&gt; [1] 276 #&gt; #&gt; $n.exp1 #&gt; [1] 138 #&gt; #&gt; $n.exp0 #&gt; [1] 138 #&gt; #&gt; $power #&gt; [1] 0.8 #&gt; #&gt; $irr #&gt; [1] 1.75 #&gt; #&gt; $or #&gt; [1] 2.153846 Note: It doesn’t matter if you swap the proportions for the exposed and unexposed groups, i.e. the command epi.sscohortc(irexp1=0.2, irexp0=0.35, n=NA, power=0.8) gives the same results. 6.3 Sample size calculation with a relative risk The epiR package does not have a function to estimate sample size and power directly for a relative risk, but we can use the epi.sscohortc function. 6.4 Sample size calculation with an odds ratio We can use the epi.sscc function to calculate a sample size based on an odds ratio in a case-control study: epi.sscc(OR, p1 = NA, p0, n, power, r = 1, phi.coef = 0, design = 1, sided.test = 2, nfractional = FALSE, conf.level = 0.95, method = &quot;unmatched&quot;, fleiss = FALSE) Using information from Worked Example 10.4, we specify: OR: the odds ratio to be detected, here 1.5 p0: the proportion of the outcome in the controls, here 0.5 n: the sample size, here to be calculated power: the required study power, here 0.9 epi.sscc(OR=1.5, p0=0.5, n=NA, power=0.9) #&gt; $n.total #&gt; [1] 1038 #&gt; #&gt; $n.case #&gt; [1] 519 #&gt; #&gt; $n.control #&gt; [1] 519 #&gt; #&gt; $power #&gt; [1] 0.9 #&gt; #&gt; $OR #&gt; [1] 1.5 Now we calculate the sample size for Worked Example 10.5: epi.sscc(OR=2, p0=0.3, n=NA, power=0.9) #&gt; $n.total #&gt; [1] 376 #&gt; #&gt; $n.case #&gt; [1] 188 #&gt; #&gt; $n.control #&gt; [1] 188 #&gt; #&gt; $power #&gt; [1] 0.9 #&gt; #&gt; $OR #&gt; [1] 2 Here we see that we require a total of 376 participants to detect an odds ratio of 2.0 with 90% power; epi.sscc(OR=2, p0=0.3, n=NA, power=0.8) #&gt; $n.total #&gt; [1] 282 #&gt; #&gt; $n.case #&gt; [1] 141 #&gt; #&gt; $n.control #&gt; [1] 141 #&gt; #&gt; $power #&gt; [1] 0.8 #&gt; #&gt; $OR #&gt; [1] 2 or a total of 282 participants to detect an odds ratio of 2.0 with 80% power. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
