# Probability and probability distributions: R notes

## Importing data into Stata
We have described previously how to import data that have been saved as Stata .dta files. It is quite common to have data saved in other file types, such as Microsoft Excel, or plain text files. In this section, we will demonstrate how to import data from other packages into R.

### Importing plain text data into Stata
A `csv` file, or a "comma separated variables" file is commonly used to store data. These files have a very simple structure: they are plain text files, where data are separated by commas. csv files have the advantage that, as they are plain text files, they can be opened by a large number of programs (such as Notepad in Windows, TextEdit in MacOS, Microsoft Excel - even Microsoft Word). While they can be opened by Microsoft Excel, they can be opened by many other programs: the csv file can be thought of as the lingua-franca of data.

In this demonstration, we will use data on the weight of 1000 people entered in a csv file called `weight_s2.csv` available on Moodle.

To confirm that the file is readable by any text editor, here are the first ten lines of the file, opened in Notepad on Microsoft Windows, and TextEdit on MacOS.
 	 

\includegraphics[width=0.66\linewidth]{img/mod02/import-01} 

We can use the `read.csv` function:


```r
library(tidyverse)
#> -- Attaching packages ------------------- tidyverse 1.3.1 --
#> v ggplot2 3.3.5     v purrr   0.3.4
#> v tibble  3.1.6     v dplyr   1.0.8
#> v tidyr   1.2.0     v stringr 1.4.0
#> v readr   2.1.2     v forcats 0.5.1
#> -- Conflicts ---------------------- tidyverse_conflicts() --
#> x dplyr::filter() masks stats::filter()
#> x dplyr::lag()    masks stats::lag()
library(jmv)

weights <- read.csv("data/examples/Weight_s2.csv")
```

Here, the `read.csv` function has the default that the first row of the dataset contains the variable names. If your data do not have column names, you can use `header=FALSE` in the function.

Note: there is an alternative function `read_csv` which is part of the `readr` package (a component of the `tidyverse`). Some would argue that the `read_csv` function is more appropriate to use because of an issue known as `strings.as.factors`. The `strings.as.factors` default was removed in R Version 4.0.0, so it is less important which of the two functions you use to import a `.csv` file. More information about this issue can be found [here](https://simplystatistics.org/posts/2015-07-24-stringsasfactors-an-unauthorized-biography) and [here](https://developer.r-project.org/Blog/public/2020/02/16/stringsasfactors/).

## Checking your data for errors in Stata
Before you start describing and analysing your data, it is important to make sure that no errors have been made during the data entry process. Basically, you are looking for values that are outside the range of possible or plausible values for that variable.

If an error is found, the best method for correcting the error is to go back to the original data e.g. the hard copy questionnaire, to obtain the original value, entering the correct value into R If the original data is not available or the original data is also incorrect, the erroneous value is often excluded from the dataset. 

For continuous variables, the easiest methods are to examine a boxplot and histogram. For example, a boxplot and histogram for the weight variable we just imported appear as:


```r
hist(weights$weight, xlab="Weight (kg)", main="Histogram of 1000 weights")
```

![](02.1-Probability_files/figure-latex/unnamed-chunk-3-1.pdf)<!-- --> 

```r
boxplot(weights$weight, xlab="Weight (kg)", main="Boxplot of 1000 weights")
```

![](02.1-Probability_files/figure-latex/unnamed-chunk-3-2.pdf)<!-- --> 

There is a clear outlying point shown in the boxplot. Although not obvious, the same point is shown in the histogram as a bar around 700 with a very short height.

We can identify any outlying observations in the dataset using the `filter` function, loaded with the `tidyverse`. You will need to decide if these values are a data entry error or are biologically plausible. If an extreme value or "outlier", is biologically plausible, it should be included in all analyses.

For example, to list any observations from the `weights` dataset with a weight larger than 200:


```r
dplyr::filter(weights, weight>200)
#>   id weight
#> 1 58  700.2
```

We see that there is a very high value of 700.2kg. A value as high as 700kg is likely to be a data entry error (e.g. error in entering an extra zero) and is not a plausible weight value. Here, **you should check your original data**.

You might find that the original weight was recorded in medical records as 70.2kg. You can change this in R by writing code.

**Note:** many statistical packages, including Stata, will allow you to view a spreadsheet version of your data and edit values in that spreadsheet. This is not best practice, as corrected observations may revert to their original values depending on whether the edited data have been saved or not. By using code-based recoding, the changes will be reproduced the next time the code is run.

We will use an `ifelse` statement to recode the incorrect weight of 700.2kg into 70.2kg. The form of the `ifelse` statement is as follows:

`ifelse(test, value_if_true, value_if_false)`

Our code will create a new column (called `weight_clean`) in the `weights` dataframe. We will test whether `weight` is equal to 700.2; if this is true, we will assign `weight_clean` to be 70.2, otherwise `weight_clean` will equal the value of `weight`. Putting it all together:


```r
weights$weight_clean = ifelse(weights$weight==700.2, 70.2, weights$weight)
```

**Note:** if an extreme value lies within the range of biological plausibility it should not be removed from analysis.

Once you have checked your data for errors, you are ready to start analysing your data.

## Overlaying a Normal curve on a histogram

It can be useful to produce a histogram with an overlayed Normal curve to assess whether our sample appears approximately Normally distributed.

### Base graphics

```r
hist(weights$weight_clean, xlab="Weight (kg)", main="Histogram of 1000 weights", probability = TRUE)
curve(dnorm(x, mean=mean(weights$weight_clean), sd=sd(weights$weight_clean)), col="darkblue", add=TRUE)
```

![](02.1-Probability_files/figure-latex/unnamed-chunk-6-1.pdf)<!-- --> 

```r

hist(weights$weight_clean, xlab="Weight (kg)", main="Histogram of 1000 weights", probability = TRUE, ylim=c(0,0.1))
curve(dnorm(x, mean=mean(weights$weight_clean), sd=sd(weights$weight_clean)), col="darkblue", add=TRUE)
```

![](02.1-Probability_files/figure-latex/unnamed-chunk-6-2.pdf)<!-- --> 

## ggplot2

```r
ggplot(weights, aes(x=weight_clean)) + geom_histogram(aes(y=..density..), breaks=seq(50, 90, 5), colour="black", fill="grey") +
  stat_function(fun = dnorm,
                args = list(mean = mean(weights$weight_clean),
                            sd = sd(weights$weight_clean)),
                colour="darkblue") +
  theme_classic() +
  labs(title="Histogram of 1000 weights", x="Weight (kg)", y="Density")
```

![](02.1-Probability_files/figure-latex/unnamed-chunk-7-1.pdf)<!-- --> 

## Descriptive statistics for checking normality
All the descriptive statistics including Skewness and Kurtosis discussed in this module can be obtained using the `descriptives` function from the `jmv` package. In particular, skewness and kurtosis can be requested over and above the default statistics (`skew=TRUE, kurt=TRUE`):


```r
descriptives(data=weights, vars=weight_clean, skew=TRUE, kurt=TRUE)
#> 
#>  DESCRIPTIVES
#> 
#>  Descriptives                            
#>  ─────────────────────────────────────── 
#>                           weight_clean   
#>  ─────────────────────────────────────── 
#>    N                              1000   
#>    Missing                           0   
#>    Mean                       69.76450   
#>    Median                     69.80000   
#>    Standard deviation         5.052676   
#>    Minimum                    53.80000   
#>    Maximum                    85.80000   
#>    Skewness                 0.07360659   
#>    Std. error skewness      0.07734382   
#>    Kurtosis                 0.05418774   
#>    Std. error kurtosis       0.1545343   
#>  ───────────────────────────────────────
```

## Importing Excel data into Stata {#import-excel}
Another common type of file that data are stored in is a Microsoft Excel file (.xls or .xlsx). In this demonstration, we will import a selection of records from a large health survey, stored in the file `health-survey.xlsx`.

The health survey data contains 1140 records, comprising:

- sex: 1 = respondent identifies as male; 2 = respondent identifies as female
- height: height in meters
- weight: weight in kilograms

To import data from Microsoft Excel, we can use the `read_excel()` function in the `readxl` package.


```r
library(readxl)

survey <- read_excel("data/examples/health-survey.xlsx")
summary(survey)
#>       sex           height          weight      
#>  Min.   :1.00   Min.   :1.220   Min.   : 22.70  
#>  1st Qu.:1.00   1st Qu.:1.630   1st Qu.: 68.00  
#>  Median :2.00   Median :1.700   Median : 79.40  
#>  Mean   :1.55   Mean   :1.698   Mean   : 81.19  
#>  3rd Qu.:2.00   3rd Qu.:1.780   3rd Qu.: 90.70  
#>  Max.   :2.00   Max.   :2.010   Max.   :213.20
```

We can see that sex has been entered as a numeric variable. We should transform it into a factor so that we can assign labels to each category:


```r
survey$sex <- factor(survey$sex, level=c(1,2), labels=c("Male", "Female"))

summary(survey$sex)
#>   Male Female 
#>    513    627
```

We also note that height looks like it has been entered as meters, and weight as kilograms.

## Generating new variables

Our health survey data contains information on height and weight. We often summarise body size using BMI: body mass index which is calculated as: $\frac{\text{weight (kg)}}{(\text{height (m)})^2}$

We can create a new column in our dataframe in many ways, and we will present two alternatives.

### Base R

A new column can be generated using the following approach:

`dataframe$new_column = <formula>`

For example:

```r
survey$bmi = survey$weight / (survey$height^2)
```


### tidyverse

Using the tidyverse approach, we use the `mutate` command to change (or create) a column of data within the dataframe:


```r
survey <- survey %>% 
  mutate(bmi = weight / (height^2))
```

We should check the construction of the new variable by examining some records, and examining a histogram and boxplot:


```r
head(survey)
#> # A tibble: 6 x 4
#>   sex    height weight   bmi
#>   <fct>   <dbl>  <dbl> <dbl>
#> 1 Male     1.63   81.7  30.8
#> 2 Male     1.63   68    25.6
#> 3 Male     1.85   97.1  28.4
#> 4 Male     1.78   89.8  28.3
#> 5 Male     1.73   70.3  23.5
#> 6 Female   1.57   85.7  34.8
tail(survey)
#> # A tibble: 6 x 4
#>   sex    height weight   bmi
#>   <fct>   <dbl>  <dbl> <dbl>
#> 1 Female   1.65   95.7  35.2
#> 2 Male     1.8    79.4  24.5
#> 3 Female   1.73   83    27.7
#> 4 Female   1.57   61.2  24.8
#> 5 Male     1.7    73    25.3
#> 6 Female   1.55   91.2  38.0

hist(survey$bmi)
```

![](02.1-Probability_files/figure-latex/unnamed-chunk-13-1.pdf)<!-- --> 

```r
boxplot(survey$bmi)
```

![](02.1-Probability_files/figure-latex/unnamed-chunk-13-2.pdf)<!-- --> 

In the general population, BMI ranges between about 15 to 30. It appears that BMI has been correctly generated in this example. We should investigate the very low and some of the very high values of BMI, but this will be left for another time.

## Summarising data by another variable

We will often want to calculate the same summary statistics by another variable. For example, we might want to calculate summary statistics for BMI for males and females separately. We can do this in in the `descriptives` function by defining sex as a `splitBy` variable:


```r
descriptives(data=survey, vars=bmi, splitBy = sex)
#> 
#>  DESCRIPTIVES
#> 
#>  Descriptives                                 
#>  ──────────────────────────────────────────── 
#>                          sex       bmi        
#>  ──────────────────────────────────────────── 
#>    N                     Male           513   
#>                          Female         627   
#>    Missing               Male             0   
#>                          Female           0   
#>    Mean                  Male      28.29561   
#>                          Female    27.81434   
#>    Median                Male      27.39592   
#>                          Female    26.66667   
#>    Standard deviation    Male      5.204975   
#>                          Female    6.380523   
#>    Minimum               Male      16.47519   
#>                          Female    9.209299   
#>    Maximum               Male      57.23644   
#>                          Female    52.59516   
#>  ────────────────────────────────────────────
```


[PLOTS BY VARIABLES]

## Recoding data {#recoding-data}
One task that is common in statistical computing is to recode variables. For example, we might want to group some categories of a categorical variable, or to present a continuous variable in a categorical way.

In this example, we can recode BMI into the following categories as suggested by the World Health Organisation [footnote]:

- Underweight: BMI < 18.5
- Normal weight: 18.5 $\le$ BMI < 25
- Pre-obesity: 25 $\le$ BMI < 30
- Obesity Class I: 30 $\le$ BMI < 35
- Obesity Class II: 35 $\le$ BMI < 40
- Obesity Class III: BMI $\ge$ 40

The quickest way to recode a continuous variable into categories is to use the `cut` command which takes a continuous variable, and "cuts" it into groups based on the specified "cutpoints":


```r
survey$bmi_cat <- cut(survey$bmi, c(0, 18.5, 25, 30, 35, 40, 100))
```

Notice that lower (BMI=0) and upper (BMI=100) bounds have been specified, as both a lower and upper limit must be defined for each group.

If we examine the new `bmi_cat` variable:

```r
summary(survey$bmi_cat)
#>  (0,18.5] (18.5,25]   (25,30]   (30,35]   (35,40]  (40,100] 
#>        18       362       411       205        97        47
```

we see that each group has been labelled (a, b]. This notation is equivalent to: greater than a, and less than or equal to b. The `cut` function excludes the lower limit, but includes the upper limit. Our BMI ranges have been defined to include the lower limit, and exclude the upper limit (for example, greater than or equal to 30 and less than 35).

We can specify this recoding using the `right=FALSE` option:


```r
survey$bmi_cat <- cut(survey$bmi, c(0, 18.5, 25, 30, 35, 40, 100), right=FALSE)
summary(survey$bmi_cat)
#>  [0,18.5) [18.5,25)   [25,30)   [30,35)   [35,40)  [40,100) 
#>        18       362       411       201       101        47
```

More complex recoding can be done using the `case_when` command in the `dplyr` package.

[INCLUDE???]

## Computing binomial probabilities using R
There are two R functions that we can use to calculate probabilities based on the binomial distribution: `dbinom` and `pbinom`:

- `dbinom(x, size, prob)` gives the probability of obtaining `x` successes from `size` trials when the probability of a success on one trial is `prob`;
- `pbinom(q, size, prob)` gives the probability of obtaining `q` **or fewer** successes from `size` trials when the probability of a success on one trial is `prob`;
- `pbinom(q, size, prob, lower.tail=FALSE)` gives the probability of obtaining **more than** `q`successes from `size` trials when the probability of a success on one trial is `prob`.

To do the computation for part (a) in Worked Example 2.1, we will use the `dbinom` function with:

- *x* is the number of successes, here, the number of smokers (i.e. k=3);
- *size* is the number of trials (i.e. n=6);
- and *prob* is probability of drawing a smoker from the population, which is 19.8% (i.e. p=0.198).

Replace each of these with the appropriate number into the formula:


```r
dbinom(x=3, size=6, prob=0.198)
#> [1] 0.08008454
```

To calculate the upper tail of probability in part (b), we use the `pbinom(lower.tail=FALSE)` function. Note that the `pbinom(lower.tail=FALSE)` function **does not include `q`**, so to obtain 4 or more successes, we need to enter `q=3`:


```r
pbinom(q=3, size=6, prob=0.198, lower.tail=FALSE)
#> [1] 0.01635325
```

For the lower tail for part (c), we use the `pbinom` function:


```r
pbinom(q=2, size=6, prob=0.198)
#> [1] 0.9035622
```


## Computing probabilities from a Normal distribution

We can use the `pnorm` function to calculate probabilities from a Normal distribution:

- `pnorm(q, mean, sd)` calculates the probability of observing a value of `q` or less, from a Normal distribution with a mean of `mean` and a standard deviation of `sd`. Note that if `mean` and `sd` are not entered, they are assumed to be 0 and 1 respectively (i.e. a standard normal distribution.)
- `pnorm(q, mean, sd, lower.tail=FALSE)` calculates the probability of observing a value of `q` or more, from a Normal distribution with a mean of `mean` and a standard deviation of `sd`.

To obtain the probability of obtaining 0.5 or greater from a standard normal distribution:


```r
pnorm(0.5, lower.tail=FALSE)
#> [1] 0.3085375
```

To calculate the worked example: Assume that the mean diastolic blood pressure for men is 77.9 mmHg, with a standard deviation of 11. What is the probability that a man selected at random will have high blood pressure (i.e. diastolic blood pressure $ge$ 90)?


```r
pnorm(90, mean=77.9, sd=11, lower.tail=FALSE)
#> [1] 0.1356661
```
